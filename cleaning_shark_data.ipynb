{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe952ab-18df-444e-812b-90833b4bbed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "shark_df = pd.read_excel('./shark-dataset.xls')\n",
    "# create dataframe copy\n",
    "original_df = shark_df.copy()\n",
    "shark_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1847c872-03ef-4dfa-9d3f-6b83fde3decc",
   "metadata": {},
   "source": [
    "Data cleaning to-do's:\n",
    "- (done) column names to lowercase\n",
    "- (done) column names - remove empty space after name\n",
    "- (done) column names - replace empty spaces with underscores\n",
    "- find unique values in columns\n",
    "- find duplicate rows\n",
    "- year names without floating\n",
    "- column 'Unnamed: 21' & 'Unnamed: 22' - what do we do with them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27d2263-6387-4a3c-b18c-d96343596dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# column names to lowercase, remove empty space in the end, replace empty spaces with underscore\n",
    "shark_df.columns = shark_df.columns.str.lower().str.strip().str.replace(\" \", \"_\")\n",
    "\n",
    "# remove multiple columns:\n",
    "shark_df = shark_df.drop(['type', 'state', 'name', 'species', 'source', 'pdf', 'href_formula', 'href', 'case_number', 'case_number.1', 'original_order', 'unnamed:_21', 'unnamed:_22', 'time'], axis=1)\n",
    "\n",
    "# convert year values to int\n",
    "# shark_df['year'] = shark_df['year'].apply(lambda x: int(x) if isinstance(x, float) else x)\n",
    "shark_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9859cbf-765e-4989-8d03-0e888445dc58",
   "metadata": {},
   "source": [
    "TUNG: YEARS\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1bca00-f9dc-451a-a166-8c3b6d046299",
   "metadata": {},
   "outputs": [],
   "source": [
    "dft = shark_df\n",
    "\n",
    "# Filter the DataFrame because years date back to the birth of jesus christ, our lord and saviour. \n",
    "start_year = 2014\n",
    "end_year = 2024\n",
    "\n",
    "dft = shark_df[(shark_df[\"year\"] >= start_year) & (shark_df[\"year\"] <= end_year)]\n",
    "\n",
    "#convert float in year to int\n",
    "#this way doesn't convert into int, but error messages disappears, WHYYYY\n",
    "dft[\"year\"] = dft[\"year\"].fillna(0).astype(int)\n",
    "\n",
    "#this conversion works, however the stupid error message \n",
    "#year_filtered_dft[\"year\"] = year_filtered_dft[\"year\"].apply(lambda x : int(x))\n",
    "\n",
    "# showing that there are NO NaNs in date and year; \n",
    "print(dft.isnull().sum())\n",
    "print(\"\\nThe unique values in year are:\\n\", dft.year.unique())\n",
    "print(\"\\nThe datatype for column is currently:\\n\", dft.year.dtype)\n",
    "\n",
    "dft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e16973-fc18-4d28-9879-5aa67fe5a96d",
   "metadata": {},
   "source": [
    "TUNG: DATES\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac2c376-b5d2-4999-913f-ca7681c8ad71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "# Function to parse different date formats\n",
    "def parse_date(date_str):\n",
    "    if isinstance(date_str, str):\n",
    "        try: \n",
    "            return pd.to_datetime(date_str)  # Try direct conversion\n",
    "        except ValueError:\n",
    "            match = re.search(r'(\\d{4}-\\d{1,2}-\\d{1,2}|\\d{1,2}-[A-Za-z]{3}-\\d{4}|\\b[A-Za-z]{3}-\\d{4}\\b)', date_str)\n",
    "            if match:\n",
    "                date_str = match.group(0)\n",
    "                try:\n",
    "                    return datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "                except ValueError:\n",
    "                    try:\n",
    "                        return datetime.strptime(date_str, \"%d-%b-%Y\")\n",
    "                    except ValueError:\n",
    "                        try:\n",
    "                            return datetime.strptime(date_str, \"%b-%Y\")\n",
    "                        except ValueError:\n",
    "                            return None  # Return None for invalid formats\n",
    "    elif isinstance(date_str, datetime):\n",
    "        return date_str  # Return the datetime object as is\n",
    "    return None  # Return None if not a string or datetime\n",
    "\n",
    "# Create datetime_column and string_column\n",
    "dft[\"datetime_column\"] = dft[\"date\"].apply(parse_date)\n",
    "dft[\"string_column\"] = dft[\"date\"].apply(lambda x: x if isinstance(x, str) else None)\n",
    "\n",
    "# Drop rows with invalid datetime values\n",
    "dft = dft[dft[\"datetime_column\"].notna()]\n",
    "\n",
    "# Extract month and year from datetime_column\n",
    "dft['month'] = dft[\"datetime_column\"].apply(lambda x: x.month if pd.notnull(x) else None)\n",
    "dft['year'] = dft[\"datetime_column\"].apply(lambda x: x.year if pd.notnull(x) else None)\n",
    "#dft['month'] = dft[\"datetime_column\"].dt.month\n",
    "#dft['year'] = dft[\"datetime_column\"].dt.year\n",
    "\n",
    "# Define season mapping\n",
    "season_mapping = {\n",
    "    \"Spring\": [3, 4, 5],\n",
    "    \"Summer\": [6, 7, 8],\n",
    "    \"Autumn\": [9, 10, 11],\n",
    "    \"Winter\": [12, 1, 2]\n",
    "}\n",
    "\n",
    "# Function to assign season based on month\n",
    "def what_season(month):\n",
    "    for season, months in season_mapping.items():\n",
    "        if month in months:\n",
    "            return season\n",
    "    return None\n",
    "\n",
    "# Assign season based on the extracted month\n",
    "dft['season'] = dft['month'].apply(what_season)\n",
    "\n",
    "# Check the resulting DataFrame\n",
    "print(dft[['date', 'datetime_column', 'string_column', 'year', 'month', 'season']])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a18b41b-1fe8-41e6-9689-7592d0da6bc6",
   "metadata": {},
   "source": [
    "extras\n",
    "\n",
    "groups_by_seanson_and_injury = dft.groupby([\"season\", \"injury\"])\n",
    "groups_by_seanson_and_injury.get_group(\"Autumn\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127ad7a9",
   "metadata": {},
   "source": [
    "# Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d6e283-fea9-43fe-806d-852d29d169ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Johanna - country and location\n",
    "dfj = shark_df.copy()\n",
    "\n",
    "# unique countries\n",
    "dfj.country.unique() # many many countries (unsure values and very small countries)\n",
    "top_20_countries = dfj.country.value_counts().head(20)\n",
    "\n",
    "# stick to top 20 countries\n",
    "dfj = dfj[dfj.country.isin(top_20_countries.index)] # deletes around 1000 of originally 6900 rows\n",
    "\n",
    "# first letter in uppercase for all countries except USA for consistency\n",
    "dfj.country = dfj.country.apply(lambda x: x.title() if x != 'USA' else x)\n",
    "dfj.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c21dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop location column\n",
    "dfj_USA = dfj[dfj.country.isin(['USA'])]\n",
    "dfj_USA.head()\n",
    "dfj_USA.location.unique() # too complicated --> stick to USA overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55548536",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a62324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=top_20_countries.index.str.title(), y=top_20_countries.values, palette='viridis')\n",
    "plt.title('Top 20 countries of shark attacks')\n",
    "plt.xlabel('Country')\n",
    "plt.tick_params(axis='x', labelsize=9)\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac74d624-17d7-470a-8c72-f5e25b4966d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jonathan - sex wrangling\n",
    "\n",
    "#my copy of df\n",
    "jon_df = shark_df.copy()\n",
    "\n",
    "#check unique values\n",
    "unique_values_sex = jon_df['sex'].value_counts(dropna=False)\n",
    "print(f\"Unique values before cleaning: {unique_values_sex}\")\n",
    "\n",
    "# Remove extra spaces\n",
    "jon_df['sex'] = jon_df['sex'].str.strip()\n",
    "\n",
    "# Replace specific values\n",
    "jon_df['sex'] = jon_df['sex'].replace({\n",
    "    'M': 'M',  # Standard \"M\"\n",
    "    'F': 'F',  # Standard \"F\"\n",
    "    'N': np.nan,  # Likely an error or placeholder, so set as NaN\n",
    "    'M x 2': 'M',  # Assuming this means \"male,\" so map to \"M\"\n",
    "    'lli': np.nan,  # Unrecognized value, replace with NaN\n",
    "    '.': np.nan,  # Unrecognized value, replace with NaN\n",
    "    ' M': 'M'  # Correct extra space for \"M\"\n",
    "})\n",
    "\n",
    "jon_df['sex']= jon_df['sex'].fillna('unknown')\n",
    "\n",
    "#replaced NaN as unknown for now? Suggestion: change the unknown values to M or F based on the percentage of M and F we do know. \n",
    "# eg. total M is 88% of total, and F is 12%. Apply those percentages to the unknown values.\n",
    "\n",
    "# Step 1: Calculate the counts of \"M\" and \"F\"\n",
    "#total_known = jon_df['sex'].value_counts()\n",
    "#m_count = total_known['M']\n",
    "#f_count = total_known['F']\n",
    "#total = m_count + f_count\n",
    "\n",
    "# Step 2: Calculate the percentages of \"M\" and \"F\"\n",
    "#m_percentage = m_count / total\n",
    "#f_percentage = f_count / total\n",
    "\n",
    "# Step 3: Determine the number of \"Unknown\" values\n",
    "#unknown_count = jon_df['sex'].value_counts().get('Unknown', 0)\n",
    "\n",
    "# Step 4: Calculate how many \"Unknown\" values to fill with \"M\" and \"F\"\n",
    "#m_fill_count = int(m_percentage * unknown_count)\n",
    "#f_fill_count = unknown_count - m_fill_count  # Remaining to ensure the total matches\n",
    "\n",
    "# Step 5: Replace \"Unknown\" values with \"M\" and \"F\" based on calculated counts\n",
    "# First, get the indices of the \"Unknown\" entries\n",
    "#unknown_indices = jon_df[jon_df['sex'] == 'Unknown'].index\n",
    "\n",
    "# Randomly sample indices for M and F replacements\n",
    "#m_indices = np.random.choice(unknown_indices, size=m_fill_count, replace=False)\n",
    "#f_indices = unknown_indices.difference(m_indices)\n",
    "\n",
    "# Assign \"M\" to sampled indices and \"F\" to remaining indices\n",
    "#jon_df.loc[m_indices, 'sex'] = 'M'\n",
    "#jon_df.loc[f_indices, 'sex'] = 'F'\n",
    "\n",
    "jon_df['sex']= jon_df['sex'].fillna('unknown')\n",
    "\n",
    "\n",
    "unique_values_sex = jon_df['sex'].value_counts(dropna=False)\n",
    "print(f\"Unique values after cleaning: {unique_values_sex}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce333174-a9d8-451e-a6fb-43cf3b929649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jonathan - age wrangling\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Set Pandas option to display all rows\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Show all unique values in the 'age' column, including NaN counts\n",
    "unique_values_age = jon_df['age'].value_counts(dropna=False)\n",
    "print(f\"Unique values before cleaning:\\n{unique_values_age}\")\n",
    "\n",
    "# Reset the display option back to default to avoid affecting other output\n",
    "pd.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a44547-be2c-4ad0-a35d-b62bdd5f51a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Step 1: Handle common descriptive terms with approximate ages\n",
    "def convert_descriptive_age(value):\n",
    "    if pd.isnull(value):\n",
    "        return np.nan\n",
    "    value = str(value).strip().lower()\n",
    "    if value in [\"teen\", \"teens\"]:\n",
    "        return 15  # Approximate age for teenagers\n",
    "    elif value == \"adult\":\n",
    "        return 30  # General average for adult age\n",
    "    elif value in [\"middle age\", '\"middle-age\"']:\n",
    "        return 45  # Approximate age for middle age\n",
    "    elif value == \"elderly\":\n",
    "        return 70  # Approximate age for elderly\n",
    "    elif value in [\"a minor\", \"young\"]:\n",
    "        return 10  # Assume a minor is around 10 years old\n",
    "    elif value == \"infant\" or value == \"9 months\" or value == \"2 to 3 months\":\n",
    "        return 1  # Age 1 for infants\n",
    "    elif \"month\" in value:\n",
    "        return 1  # Treat other month values as infants\n",
    "    return value\n",
    "\n",
    "jon_df['age'] = jon_df['age'].apply(convert_descriptive_age)\n",
    "\n",
    "def convert_to_first_age(value):\n",
    "    if isinstance(value, str):\n",
    "        # Extract first number found in the string, ignoring the rest\n",
    "        numbers = re.findall(r'\\d+', value)\n",
    "        if numbers:\n",
    "            return int(numbers[0])  # Use the first number as the age\n",
    "    return value\n",
    "\n",
    "jon_df['age'] = jon_df['age'].apply(convert_to_first_age)\n",
    "\n",
    "def convert_half_age(value):\n",
    "    if isinstance(value, str) and \"½\" in value:\n",
    "        # Replace \"½\" with \".5\" and convert to float\n",
    "        return float(value.replace(\"½\", \".5\"))\n",
    "    return value  # Return unchanged if not fractional\n",
    "\n",
    "jon_df['age'] = jon_df['age'].apply(convert_half_age)\n",
    "\n",
    "\n",
    "def convert_irregular_entries(value):\n",
    "    if isinstance(value, str):\n",
    "        # If the string contains no digits at all, set it to NaN\n",
    "        if not any(char.isdigit() for char in value):\n",
    "            return np.nan\n",
    "    return value\n",
    "\n",
    "jon_df['age'] = jon_df['age'].apply(convert_irregular_entries)\n",
    "\n",
    "jon_df['age'] = jon_df['age'].astype('Int64')\n",
    "\n",
    "# Replace <NA> with a specific integer value\n",
    "jon_df['age'] = jon_df['age'].fillna(0)  # Replaces <NA> with 0, for the sake of easier manipulation?\n",
    "\n",
    "# Set Pandas option to display all rows\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Show all unique values in the 'age' column, including NaN counts\n",
    "unique_values_age = jon_df['age'].value_counts(dropna=False)\n",
    "print(f\"Unique values after cleaning:\\n{unique_values_age}\")\n",
    "\n",
    "# Reset the display option back to default to avoid affecting other output\n",
    "pd.reset_option('display.max_rows')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980d07f3-2175-44fb-a309-ea2968fc9a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "gender_counts = jon_df['sex'].value_counts()\n",
    "print(gender_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec8d7f6-44f1-4be6-a631-1c35660295e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='sex', data=jon_df, order=gender_counts.index)\n",
    "plt.title('Shark Attacks by Gender')\n",
    "plt.xlabel('Gender')\n",
    "plt.ylabel('Number of Attacks')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24786aa0-b45f-4275-b26e-13fcd68e5ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_bins = [0, 17, 30, 50, 100]  # Define bins for age ranges\n",
    "age_labels = ['0-17', '18-30', '31-50', '51+']\n",
    "jon_df['age_group'] = pd.cut(jon_df['age'], bins=age_bins, labels=age_labels)\n",
    "\n",
    "age_group_counts = jon_df['age_group'].value_counts().sort_index()\n",
    "print(age_group_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0820460-c043-4e65-8ff3-405823a53945",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='age_group', data=jon_df, order=age_group_counts.index)\n",
    "plt.title('Shark Attacks by Age Group')\n",
    "plt.xlabel('Age Group')\n",
    "plt.ylabel('Number of Attacks')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5102456b-3d45-430a-bd8e-650ec72e5486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bru - fatality wrangling\n",
    "\n",
    "#copy df for safety reasons\n",
    "bru_df = shark_df.copy()\n",
    "\n",
    "# rename column\n",
    "bru_df.rename(columns={'unnamed:_11': 'fatal'}, inplace=True)\n",
    "\n",
    "# check unique values\n",
    "unique_values_fatal = bru_df['fatal'].unique()\n",
    "print(f\"Unique values before cleaning: {unique_values_fatal}\")\n",
    "\n",
    "# replace values\n",
    "bru_df['fatal'] = bru_df['fatal'].replace({'N':'no','Y':'yes','n':'no','Y x 2':'yes',' N':'no','N ':'no','y':'yes','UNKNOWN':'unknown'})\n",
    "\n",
    "# remove rows with specific values\n",
    "rows_to_remove = ['Nq', 'M', 'F', 2017]\n",
    "bru_df = bru_df[~bru_df['fatal'].isin(rows_to_remove)]\n",
    "\n",
    "#drop rows with NaN values\n",
    "bru_df = bru_df.dropna(subset=['fatal'])\n",
    "\n",
    "# check again\n",
    "unique_values_fatal = bru_df['fatal'].unique()\n",
    "print(f\"Unique values after cleaning: {unique_values_fatal}\")\n",
    "\n",
    "# print(bru_df.describe())\n",
    "print(bru_df.groupby('fatal').count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9047b21-de74-4080-9a95-b81921030d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bru - activity wrangling\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# check unique values\n",
    "unique_values_activity = bru_df['activity'].unique()\n",
    "# print(unique_values_activity)\n",
    "\n",
    "# convert all values to a common case\n",
    "bru_df['activity'] = bru_df['activity'].str.strip().str.lower().str.replace(r\"[\\\"']\", '', regex=True)\n",
    "\n",
    "'''\n",
    "The word_count() function uses collections and re to concatenate all the values in the column to a single string.\n",
    "Then it will split the combined string into individual words.\n",
    "Using Counters, it will return the frequency of each word.\n",
    "It will also store the top 10 most common words in a list\n",
    "'''\n",
    "\n",
    "most_common_words = []\n",
    "\n",
    "def word_count():\n",
    "    bru_df['activity'] = bru_df['activity'].fillna('').astype(str) # replace NaN values with an empty string and convert all to string\n",
    "    all_text = ' '.join(bru_df['activity']) # combine all values into a single string\n",
    "    words = re.findall(r'\\w+', all_text.lower()) # split into words (regex to handle punctuation)\n",
    "    word_counts = Counter(words) # count word frequency\n",
    "    most_common_words = [word for word, count in word_counts.most_common(50) if len(word) >= 5]\n",
    "    return most_common_words\n",
    "    return word_count\n",
    "\n",
    "word_count()\n",
    "most_common_words = word_count()\n",
    "\n",
    "#print(\"Most common words\")\n",
    "#print(most_common_words)\n",
    "\n",
    "print('Top values before replacement function\\n', bru_df['activity'].value_counts().head(10))\n",
    "#manually entered seoelcted values\n",
    "selected_values_to_replace = ['surfing', 'diving', 'fishing', 'swimming', 'wading', 'bathing', 'snorkeling', 'kayaking', 'body boarding', 'scuba diving']\n",
    "\n",
    "\n",
    "def replace_values():\n",
    "    for word_to_replace in selected_values_to_replace:\n",
    "        bru_df.loc[bru_df['activity'].str.contains(word_to_replace, case=False, na=False), 'activity'] = word_to_replace\n",
    "\n",
    "    return bru_df\n",
    "\n",
    "updated_bru_df = replace_values()\n",
    "\n",
    "print('\\nTop values after replacement:\\n', updated_bru_df['activity'].value_counts().head(10))\n",
    "\n",
    "# print(\"Word count:\")\n",
    "# print(word_counts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee3736d-b85d-40b0-8577-e3ae97134117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# playing with visual representation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Count top 10 values in the 'activity' column\n",
    "top_values = updated_bru_df['activity'].str.capitalize().value_counts().head(10)\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=top_values.index, y=top_values.values, palette='viridis')\n",
    "plt.title('Top 10 activities that may result in shark attacks')\n",
    "plt.xlabel('Activity')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d433dec9-8c05-4dc9-a863-7be989b3dec6",
   "metadata": {},
   "source": [
    "### Columns we need in the project\n",
    "- Years\n",
    "- date (months?)\n",
    "- activity (can we stand on a beach?)\n",
    "- country/state/location -> do we need all? let's start with country + location and see how messy it gets from then on?\n",
    "- age - hospital costs might change\n",
    "- sex\n",
    "- unnamed 11 (fatal or not) yes for fatal\n",
    "- injury (how does that combine with activity type?)\n",
    "- Jonathan adds column with organs lost\n",
    "\n",
    "### Formula\n",
    "probably looks like : who are you, where are you, what are you going to do and when? -> probability calculated through that (will it be fatal or not), and then the result can be checked into the three insurance categories\n",
    "\n",
    "- location is going to be an important parameter here (another correlation to definitely check, big influential factor?)\n",
    "\n",
    "- !POINT SYSTEM!\n",
    "\n",
    "- add point system for column unique values (genius)\n",
    "\n",
    "### Other parameters to take into account:\n",
    "- comparison between years to calculate potential drama in upcoming years\n",
    "- calculate probability based on different parameters\n",
    "- we're g etting creepy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b8fdf9-d2dc-4b6f-87fe-3de251efea1d",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "1. Select columns we will use\n",
    "2. Inspect column data, see where we have null or empty values\n",
    "3. Decide what to do with the above\n",
    "4. 'homogenize' column values\n",
    "5. Figure out calculations for probability. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
